import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer
from torch.nn.utils.rnn import pad_sequence

class XSSDataset(Dataset):
    def __init__(self, data_file, tokenizer, max_length):
        self.data = []
        with open(data_file, "r", encoding="utf-8") as f:
            for line in f:
                self.data.append(line.strip())
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        inputs = self.tokenizer.encode(self.data[idx], return_tensors='pt', truncation=True, max_length=self.max_length)
        return inputs.squeeze(0)

# Hàm để pad batch
def collate_fn(batch):
    # Pad các tensor trong batch để có cùng độ dài
    return pad_sequence(batch, batch_first=True, padding_value=0)

# Tải tokenizer GPT2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
max_length = 128  # Độ dài tối đa cho các câu XSS

# Tạo dataset
train_dataset = XSSDataset("D:/Users/Desktop/RL-Pentest-WAF/Datasets/XSS/small_XSS_Dataset.txt", tokenizer, max_length)

# Tạo DataLoader với hàm collate
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)


from transformers import GPT2LMHeadModel, AdamW
from tqdm import tqdm

# Tải mô hình pretrain
model = GPT2LMHeadModel.from_pretrained("D:/Users/Desktop/RL-Pentest-WAF/Inference/models/pretrain_model_xss/pretrain_model_xss")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)  # Chuyển mô hình lên GPU nếu có

# In ra thiết bị đang được sử dụng
print(f"Model is using: {device}")

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Hàm fine-tune model
def train_model(model, train_loader, optimizer, epochs=1):
    model.train()
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        epoch_loss = 0
        for batch in tqdm(train_loader):
            batch = batch.to(device)  # Chuyển batch lên GPU
            outputs = model(batch, labels=batch)
            loss = outputs.loss
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        print(f"Loss after epoch {epoch + 1}: {epoch_loss / len(train_loader)}")

# Fine-tune model
train_model(model, train_loader, optimizer)

# Lưu mô hình fine-tune
model.save_pretrained("D:/Users/Desktop/RL-Pentest-WAF/results/fine_tuned_xss_model_clap")
tokenizer.save_pretrained("D:/Users/Desktop/RL-Pentest-WAF/results/fine_tuned_xss_model_clap")

# Tải lại model fine-tune
fine_tuned_model = GPT2LMHeadModel.from_pretrained("D:/Users/Desktop/RL-Pentest-WAF/results/fine_tuned_xss_model_clap")
fine_tuned_model = fine_tuned_model.to(device)
fine_tuned_model.eval()

# Tạo response từ model fine-tune
def generate_xss(input_text, model, tokenizer, max_length=50):
    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)  # Chuyển input lên GPU
    output = model.generate(input_ids, max_length=max_length, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)
    return tokenizer.decode(output[0], skip_special_tokens=True)

input_text = "<element contenteditable onpointerout="
response = generate_xss(input_text, fine_tuned_model, tokenizer)
print("Generated XSS Payload: ", response)